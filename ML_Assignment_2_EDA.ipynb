{
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9QCcXhdGqSwJ"
      },
      "id": "9QCcXhdGqSwJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0876b8da",
      "metadata": {
        "id": "0876b8da"
      },
      "source": [
        "# Final Exploratory Data Analysis (EDA)\n",
        "Dataset: covtype.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YS474DzxT0Ms"
      },
      "id": "YS474DzxT0Ms",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa333f97",
      "metadata": {
        "id": "fa333f97"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set()\n",
        "plt.style.use(\"default\")\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/covtype.csv\")\n",
        "df.head()\n",
        "df=df.head(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1577feaf",
      "metadata": {
        "id": "1577feaf"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6522d357",
      "metadata": {
        "id": "6522d357",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ae25919",
      "metadata": {
        "id": "8ae25919",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "282aae55",
      "metadata": {
        "id": "282aae55",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "033332bc",
      "metadata": {
        "id": "033332bc"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97a10be4",
      "metadata": {
        "id": "97a10be4"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = df.drop_duplicates()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97914cce",
      "metadata": {
        "id": "97914cce",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "df[\"Cover_Type\"].value_counts()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a24f7184",
      "metadata": {
        "id": "a24f7184"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x=\"Cover_Type\", data=df)\n",
        "plt.title(\"Distribution of Cover Types\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a1475c5",
      "metadata": {
        "id": "6a1475c5"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_cols = df.select_dtypes(include=np.number).columns\n",
        "df[num_cols].hist(figsize=(15,12), bins=30)\n",
        "plt.suptitle(\"Histogram of Numerical Features\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bba1641a",
      "metadata": {
        "id": "bba1641a"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(14,10))\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr, cmap=\"coolwarm\", linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "788c061c",
      "metadata": {
        "id": "788c061c"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.boxplot(x=\"Cover_Type\", y=\"Elevation\", data=df)\n",
        "plt.title(\"Elevation vs Cover Type\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corr_with_target = df.corr()[\"Cover_Type\"].sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "corr_with_target.drop(\"Cover_Type\").plot(kind=\"bar\")\n",
        "plt.title(\"Correlation of Features with Cover_Type\")\n",
        "plt.ylabel(\"Correlation value\")\n",
        "plt.xlabel(\"Features\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zvtxqyYUUzul"
      },
      "id": "zvtxqyYUUzul",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,10))\n",
        "sns.heatmap(df.corr()[[\"Cover_Type\"]], annot=True, cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation of Features with Cover_Type\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qWzzjyxAVDcs"
      },
      "id": "qWzzjyxAVDcs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_summary(df):\n",
        "    num_df = df.select_dtypes(include=np.number)\n",
        "    return num_df.describe()\n",
        "\n",
        "def categorical_summary(df):\n",
        "    cat_df = df.select_dtypes(exclude=np.number)\n",
        "    return cat_df.describe()\n",
        "\n",
        "def skewness(df):\n",
        "    return df.select_dtypes(include=np.number).skew()\n",
        "def target_distribution(df, target):\n",
        "    print(df[target].value_counts())\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.countplot(x=df[target])\n",
        "    plt.title(\"Target Distribution\")\n",
        "    plt.show()\n",
        "def boxplots(df):\n",
        "    num_cols = df.select_dtypes(include=np.number).columns\n",
        "    for col in num_cols:\n",
        "        plt.figure(figsize=(5,3))\n",
        "        sns.boxplot(x=df[col])\n",
        "        plt.title(col)\n",
        "        sns.despine()\n",
        "        plt.show()\n",
        "def binary_feature_counts(df, columns, title):\n",
        "    counts = df[columns].sum().sort_values(ascending=False)\n",
        "    plt.figure(figsize=(10,5))\n",
        "    counts.plot(kind=\"bar\")\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    return counts\n",
        "#binary_feature_counts(data, wilderness_cols, \"Wilderness Area Count\")\n",
        "#binary_feature_counts(data, soil_cols, \"Soil Type Count\")\n",
        "def violin_plots(df, features, target):\n",
        "    for col in features:\n",
        "        plt.figure(figsize=(6,4))\n",
        "        sns.violinplot(x=target, y=col, data=df)\n",
        "        plt.title(col)\n",
        "        plt.show()\n",
        "\n",
        "def violin(data):\n",
        "  # plot bg\n",
        "  # Extracting all numerical features from data\n",
        "  num_fea = data.iloc[:, :10]\n",
        "\n",
        "  # extracting all binary features from data\n",
        "  binary_fea = data.iloc[:, 10:-1]\n",
        "\n",
        "  # Splitting\n",
        "  Wild_data, Soil_data = binary_fea.iloc[:,:4], binary_fea.iloc[:,4:]\n",
        "  sns.set_style(\"darkgrid\", {'grid.color': '.1'})\n",
        "\n",
        "  # setting target variable\n",
        "  target = data['Cover_Type']\n",
        "  # features to be compared with target variable\n",
        "  features = Wild_data.columns\n",
        "\n",
        "\n",
        "  # loop for plotting Violin Plot for each features in the data\n",
        "  for i in range(0, len(features)):\n",
        "\n",
        "      #figure size\n",
        "      plt.subplots(figsize=(13, 9))\n",
        "\n",
        "      # Plot violin for i feature for every class in target\n",
        "      sns.violinplot(data = Wild_data, x=target, y = features[i])\n",
        "\n",
        "      # x-axis label size\n",
        "      plt.xticks(size = 15)\n",
        "      # y-axis label size\n",
        "      plt.yticks(size = 16)\n",
        "\n",
        "      # Horizontal axis Label\n",
        "      plt.xlabel('Forest Cover Types', size = 17)\n",
        "      # Vertical axis Label\n",
        "      plt.ylabel(features[i], size = 17)\n",
        "\n",
        "      # display plot\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "LHeOVUF3ruRE"
      },
      "id": "LHeOVUF3ruRE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lMOkPhozulKC"
      },
      "id": "lMOkPhozulKC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def feature_importance(df, target, model=None):\n",
        "    X = df.drop(target, axis=1)\n",
        "    y = df[target]\n",
        "\n",
        "    if model is None:\n",
        "        model = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
        "\n",
        "    model.fit(X, y)\n",
        "    imp = pd.DataFrame({\n",
        "        \"Feature\": X.columns,\n",
        "        \"Importance\": model.feature_importances_\n",
        "    }).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "    return imp\n"
      ],
      "metadata": {
        "id": "G7TqbVcmr_1c"
      },
      "id": "G7TqbVcmr_1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=df\n",
        "\n",
        "numerical_summary(data)\n",
        "\n",
        "skewness(data)\n",
        "\n",
        "target_distribution(data, \"Cover_Type\")\n",
        "\n",
        "boxplots(data)\n",
        "\n",
        "importance = feature_importance(data, \"Cover_Type\")\n",
        "importance.head(10)\n",
        "\n",
        "#violin_plots(df, features, target)\n",
        "violin(df)\n",
        "'''\n",
        "X_train, X_test, y_train, y_test = scale_and_split(data, \"Cover_Type\")\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model_evaluation(RandomForestClassifier(n_jobs=-1), X_train, X_test, y_train, y_test)\n",
        "'''"
      ],
      "metadata": {
        "id": "7U7nXWp2sH7u"
      },
      "id": "7U7nXWp2sH7u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def basic_info(df):\n",
        "    print(df.info())\n",
        "    print(df.describe())\n",
        "\n",
        "def check_missing(df):\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "def plot_target_distribution(df, target):\n",
        "    sns.countplot(x=target, data=df)\n",
        "    plt.title(\"Target Distribution\")\n",
        "    plt.show()\n",
        "\n",
        "def correlation_plot(df):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(df.corr(), cmap=\"coolwarm\", annot=False)\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "j4P-l7-4dQoI"
      },
      "id": "j4P-l7-4dQoI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QwF_OIX5UEKT"
      },
      "id": "QwF_OIX5UEKT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    cross_val_score\n",
        ")\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "cont_names = [\n",
        "  \"Elevation\",\n",
        "  \"Aspect\",\n",
        "  \"Slope\",\n",
        "  \"R_Hydrology\",\n",
        "  \"Z_Hydrology\",\n",
        "  \"R_Roadways\",\n",
        "  \"Hillshade_9am\",\n",
        "  \"Hillshade_Noon\",\n",
        "  \"Hillshade_3pm\",\n",
        "  \"R_Fire_Points\",\n",
        "  ] # Continuous variables\n",
        "val_data=True\n",
        "area_names = ['WArea_' + str(i + 1) for i in range(4)]\n",
        "soil_names = ['Soil_' + str(i + 1) for i in range(40)]\n",
        "cat_names = area_names + soil_names # Categorical variables\n",
        "target = 'Cover_Type'\n",
        "names = cont_names + cat_names # All column names except target\n",
        "\n",
        "#  df = pd.read_csv(path, header=None, names=names+[target])\n",
        "X, y = df.iloc[:,:-1], df.iloc[:,-1]\n",
        "y = y - 1 # Change the target values to start from 0\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0)\n",
        "sc = StandardScaler().fit(X_train)\n",
        "X_train_scaled = sc.transform(X_train)\n",
        "if val_data:\n",
        "  X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, shuffle=True, random_state=1)\n",
        "  X_val_scaled = sc.transform(X_val)\n",
        "else:\n",
        "  X_val_scaled = None\n",
        "  y_val = None\n",
        "X_test_scaled = sc.transform(X_test)\n",
        "print(X_train_scaled.size)"
      ],
      "metadata": {
        "id": "r5rLQxSKUm9-"
      },
      "id": "r5rLQxSKUm9-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing a simple heuristic model\n",
        "class Heuristic:\n",
        "\n",
        "    \"\"\"A simple heuristic model that always predicts the most common class.\n",
        "        args:\n",
        "            X: feature matrix\n",
        "            y: target vector\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, X, y):\n",
        "        \"\"\"Initializes the model\"\"\"\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def predict(self, x):\n",
        "        \"\"\"Predicts the most common class\"\"\"\n",
        "        return self.y.value_counts().index[0]\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        \"\"\"Calculates the accuracy of the model\"\"\"\n",
        "        return np.mean(y == self.predict(X))"
      ],
      "metadata": {
        "id": "VPsY814vYyJi"
      },
      "id": "VPsY814vYyJi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic regression training\n",
        "def train_logistic_regression_model(X_train, y_train):\n",
        "\n",
        "    \"\"\"Trains a logistic regression model and saves it to working directory.\n",
        "        args:\n",
        "            X_train: training feature matrix\n",
        "            y_train: training target vector\n",
        "        returns:\n",
        "            log_model: the trained logistic regression model\n",
        "    \"\"\"\n",
        "\n",
        "    log_model = LogisticRegression(max_iter=1000)\n",
        "    log_model.fit(X_train,y_train)\n",
        "    joblib.dump(log_model, 'log_model.pkl')\n",
        "    return log_model\n"
      ],
      "metadata": {
        "id": "RT0ng76jXUMz"
      },
      "id": "RT0ng76jXUMz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A simple neural network model\n",
        "def train_nn_model(X_train, y_train, X_val, y_val, num_nodes, dropout_prob, learning_rate, batch_size, epochs):\n",
        "\n",
        "    \"\"\"Trains a neural network model and returns the model and the history object.\n",
        "        args:\n",
        "            X_train: training feature matrix\n",
        "            y_train: training target vector\n",
        "            X_val: validation feature matrix\n",
        "            y_val: validation target vector\n",
        "            num_nodes: number of nodes in each hidden layer\n",
        "            dropout_prob: dropout probability\n",
        "            learning_rate: learning rate\n",
        "            batch_size: batch size\n",
        "            epochs: number of epochs\n",
        "        returns:\n",
        "            nn_model: the trained neural network model\n",
        "            history: the history object\n",
        "    \"\"\"\n",
        "\n",
        "    nn_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(num_nodes,activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        tf.keras.layers.Dropout(dropout_prob),\n",
        "        tf.keras.layers.Dense(num_nodes,activation='relu'),\n",
        "        tf.keras.layers.Dropout(dropout_prob),\n",
        "        tf.keras.layers.Dense(len(y_train.unique()),activation='softmax')])\n",
        "    nn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = nn_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val))\n",
        "    return nn_model, history"
      ],
      "metadata": {
        "id": "LaPLZc7KYcSC"
      },
      "id": "LaPLZc7KYcSC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision tree training\n",
        "def train_dt_model(X_train, y_train):\n",
        "\n",
        "    \"\"\"Trains a Decision tree model and saves it to working directory.\n",
        "        args:\n",
        "            X_train: training feature matrix\n",
        "            y_train: training target vector\n",
        "        returns:\n",
        "            dt_model: the trained decision tree model\n",
        "    \"\"\"\n",
        "\n",
        "    dt_model = DecisionTreeClassifier()\n",
        "    dt_model.fit(X_train,y_train)\n",
        "    joblib.dump(dt_model, 'dt_model.pkl')\n",
        "    return dt_model"
      ],
      "metadata": {
        "id": "_5NnVvJtXUv5"
      },
      "id": "_5NnVvJtXUv5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the models\n",
        "def evaluate_model(model, X_test, y_test, verbose=False):\n",
        "\n",
        "    \"\"\"Loads the model and evaluates it on the test set.\n",
        "        args:\n",
        "            model_path: path to the model\n",
        "            X_test: test feature matrix\n",
        "            y_test: test target vector\n",
        "            verbose: whether to print the classification report\n",
        "        returns:\n",
        "            accuracy: the accuracy of the model\n",
        "    \"\"\"\n",
        "\n",
        "    if verbose:\n",
        "        print(f'Classification report for {model}: {classification_report(y_test,model.predict(X_test))}')\n",
        "    return accuracy_score(y_test,model.predict(X_test))\n"
      ],
      "metadata": {
        "id": "JNVuzQEoXW-K"
      },
      "id": "JNVuzQEoXW-K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning function\n",
        "def hyperparameter_tuning(X_train, y_train, X_val, y_val):\n",
        "\n",
        "    \"\"\"Tunes the hyperparameters of a neural network model.\n",
        "        args:\n",
        "            model: the neural network model\n",
        "            X_train: training feature matrix\n",
        "            y_train: training target vector\n",
        "            X_val: validation feature matrix\n",
        "            y_val: validation target vector\n",
        "        returns:\n",
        "            least_loss_model: the model with the least validation loss\n",
        "            least_loss_history: the history object of the model with the least validation loss\n",
        "    \"\"\"\n",
        "\n",
        "    least_val_loss = float('inf')\n",
        "    least_loss_model = None\n",
        "    least_loss_history = None\n",
        "    epochs = 20\n",
        "    batch_size = 128\n",
        "    for num_nodes in [32, 64]:\n",
        "        for dropout_prob in [0, 0.2]:\n",
        "            for lr in [0.005, 0.001]:\n",
        "                    print(f'{num_nodes} nodes, dropout {dropout_prob}, learning_rate {lr}, batch_size {batch_size}, ')\n",
        "                    model, history = train_nn_model(X_train,\n",
        "                                                    y_train,\n",
        "                                                    X_val,\n",
        "                                                    y_val,\n",
        "                                                    num_nodes,\n",
        "                                                    dropout_prob,\n",
        "                                                    lr,\n",
        "                                                    batch_size,\n",
        "                                                    epochs)\n",
        "                    val_loss, _ = model.evaluate(X_val, y_val)\n",
        "                    if  val_loss < least_val_loss:\n",
        "                        least_val_loss = val_loss\n",
        "                        least_loss_model = model\n",
        "                        least_loss_history = history\n",
        "    return least_loss_model, least_loss_history\n"
      ],
      "metadata": {
        "id": "OioY972kYVQy"
      },
      "id": "OioY972kYVQy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history (history):\n",
        "\n",
        "    \"\"\"Plots the training and validation loss and accuracy.\n",
        "        args:\n",
        "            history: history object returned by model.fit()\n",
        "    \"\"\"\n",
        "\n",
        "    fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,4))\n",
        "    ax1.plot(history.history['loss'], label='loss')\n",
        "    ax1.plot(history.history['val_loss'], label='val_loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.grid(True)\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(history.history['accuracy'], label='accuracy')\n",
        "    ax2.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.grid(True)\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wWILxBbDYY5K"
      },
      "id": "wWILxBbDYY5K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model, input_features):\n",
        "    \"\"\"Predicts the class of the input features.\n",
        "        args:\n",
        "            model: the trained model\n",
        "            input_features: the input features\n",
        "        returns:\n",
        "            prediction: the predicted class\n",
        "    \"\"\"\n",
        "\n",
        "    if model == 'heuristic':\n",
        "        prediction = heuristic_model.predict(input_features)\n",
        "    elif model == 'logistic_regression':\n",
        "        prediction = log_model.predict(input_features)[0]\n",
        "    elif model == 'decision_tree':\n",
        "        prediction = dt_model.predict(input_features)[0]\n",
        "    elif model == 'neural_network' and nn_model is not None:\n",
        "        prediction = np.argmax(nn_model.predict(input_features))\n",
        "    else:\n",
        "        return \"Invalid model selected\"\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "kQivSaALXbTx"
      },
      "id": "kQivSaALXbTx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ZvxbuDUD8OM"
      },
      "id": "0ZvxbuDUD8OM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2MPLZEohXkpK"
      },
      "id": "2MPLZEohXkpK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Train the models\n",
        "heuristic_model = Heuristic(X_train, y_train)\n",
        "log_model = train_logistic_regression_model(X_train, y_train)\n",
        "dt_model = train_dt_model(X_train, y_train)\n",
        "nn_model=None\n",
        "# nn_model, history = train_nn_model(X_train, y_train, X_val, y_val, 128, 0.2, 0.001, 128, 10) # Test training\n",
        "#nn_model, history = hyperparameter_tuning(X_train, y_train, X_val, y_val)\n",
        "\n",
        "if nn_model is not None:\n",
        "    plot_history(history)\n",
        "    nn_model.save('nn_model.h5')\n",
        "\n",
        "# Load trained models if necesary\n",
        "# log_model = joblib.load(\"log_model.pkl\")\n",
        "# dt_model = joblib.load(\"dt_model.pkl\")\n",
        "# nn_model = tf.keras.models.load_model('nn_model.h5')\n",
        "\n",
        "# Evaluate the models\n",
        "print('Heuristic model accuracy: ', np.round(heuristic_model.evaluate(X_test, y_test), 4)) # Heuristic model evaluation\n",
        "print('Logistic regression model accuracy: ', np.round(evaluate_model(log_model, X_test, y_test, verbose=True), 4)) # Logistic regression model evaluation\n",
        "print('Decision tree model accuracy: ', np.round(evaluate_model(dt_model, X_test, y_test, verbose=True), 4)) # Decision tree model evaluation\n",
        "if nn_model is not None:\n",
        "    nn_model.evaluate(X_test, y_test) # Neural network model evaluation\n",
        "'''\n",
        "'''\n",
        "# Testing the models\n",
        "input_features = X_test.iloc[5030,:].reshape((1,-1)) # type: ignore\n",
        "print(f'Neural network model prediction: {prediction(\"neural_network\", input_features)}')\n",
        "'''"
      ],
      "metadata": {
        "id": "9eN9Zh-SXkxC"
      },
      "id": "9eN9Zh-SXkxC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing model for feature importance\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# passing the model\n",
        "model = ExtraTreesClassifier(random_state = 53)\n",
        "\n",
        "# feeding all our features to var 'X'\n",
        "X = df.iloc[:,:-1]\n",
        "# feeding our target variable to var 'y'\n",
        "y = df['Cover_Type']\n",
        "\n",
        "# training the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# extracting feature importance from model and making a dataframe of it in descending order\n",
        "ETC_feature_importances = pd.DataFrame(model.feature_importances_, index = X.columns, columns=['ETC']).sort_values('ETC', ascending=False)\n",
        "\n",
        "# removing traces of this model\n",
        "model = None\n",
        "\n",
        "# show top 10 features\n",
        "ETC_feature_importances.head(10)"
      ],
      "metadata": {
        "id": "-EwDWEX8tNr-"
      },
      "id": "-EwDWEX8tNr-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing model for feature importance\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# passing the model\n",
        "model = RandomForestClassifier(random_state = 53)\n",
        "\n",
        "# training the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# extracting feature importance from model and making a dataframe of it in descending order\n",
        "RFC_feature_importances = pd.DataFrame(model.feature_importances_, index = X.columns, columns=['RFC']).sort_values('RFC', ascending=False)\n",
        "\n",
        "# removing traces of this model\n",
        "model = None\n",
        "\n",
        "# show top 10 features\n",
        "RFC_feature_importances.head(10)"
      ],
      "metadata": {
        "id": "pIky_8yztcjX"
      },
      "id": "pIky_8yztcjX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing model for feature importance\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# passing the model\n",
        "model = AdaBoostClassifier(random_state = 53)\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "# extracting feature importance from model and making a dataframe of it in descending order\n",
        "ADB_feature_importances = pd.DataFrame(model.feature_importances_, index = X.columns, columns=['ADB']).sort_values('ADB', ascending=False)\n",
        "\n",
        "# removing traces of this model\n",
        "model = None\n",
        "\n",
        "ADB_feature_importances.head(10)"
      ],
      "metadata": {
        "id": "qz37WNfdtfkm"
      },
      "id": "qz37WNfdtfkm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing model for feature importance\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# passing the model\n",
        "model = GradientBoostingClassifier(random_state = 53)\n",
        "\n",
        "# training the model\n",
        "model.fit(X, y)\n",
        "\n",
        "# extracting feature importance from model and making a dataframe of it in descending order\n",
        "GBC_feature_importances = pd.DataFrame(model.feature_importances_, index = X.columns, columns=['GBC']).sort_values('GBC', ascending=False)\n",
        "\n",
        "# removing traces of this model\n",
        "model = None\n",
        "\n",
        "# show top 10 features\n",
        "GBC_feature_importances.head(10)"
      ],
      "metadata": {
        "id": "ayMYBKyktjiO"
      },
      "id": "ayMYBKyktjiO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## feeding top 20 features in a variable as dataframe including target variable\n",
        "\n",
        "## AdaBoost Sample\n",
        "#sample = data[['Wilderness_Area4', 'Elevation','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Aspect','Wilderness_Area4', 'Soil_Type4', 'Soil_Type10' 'Cover_Type']]\n",
        "\n",
        "sample = df[['Elevation','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points','Horizontal_Distance_To_Hydrology','Vertical_Distance_To_Hydrology','Aspect','Wilderness_Area4',\n",
        "            'Hillshade_Noon','Hillshade_3pm','Hillshade_9am','Slope','Soil_Type22','Soil_Type10','Soil_Type4','Soil_Type34','Soil_Type34','Wilderness_Area3','Soil_Type12',\n",
        "            'Soil_Type2','Wilderness_Area1', 'Cover_Type']]"
      ],
      "metadata": {
        "id": "d5kT63AIt6Ap"
      },
      "id": "d5kT63AIt6Ap",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing feature scaling function\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# passing range to the function and then save it\n",
        "scaler = MinMaxScaler(feature_range = (0,1))\n",
        "\n",
        "# feeding sample features to var 'X'\n",
        "X = sample.iloc[:,:-1]\n",
        "\n",
        "# feeding our target variable to var 'y'\n",
        "y = sample['Cover_Type']\n",
        "\n",
        "# apply feature scaling to all features\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "VRWpAxoYt6wG"
      },
      "id": "VRWpAxoYt6wG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing train-test function\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split the data in 75%-25% train-test respectively with fixed state\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.25, random_state = 53)\n",
        "# number of training observation\n",
        "print(X_train.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "1UdNPuOot9NO"
      },
      "id": "1UdNPuOot9NO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### defining function for training models and measuring performance\n",
        "\n",
        "# to measure performance\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# for calculating time elapsed\n",
        "import time\n",
        "\n",
        "# fucntion\n",
        "def model_evaluation(clf):\n",
        "\n",
        "    # passing classifier to a variable\n",
        "    clf = clf\n",
        "\n",
        "    # records time\n",
        "    t_start = time.time()\n",
        "    # classifier learning the model\n",
        "    clf = clf.fit(X_train, y_train)\n",
        "    # records time\n",
        "    t_end = time.time()\n",
        "\n",
        "\n",
        "    # records time\n",
        "    c_start = time.time()\n",
        "    # Using 10 K-Fold CV on data, gives peroformance measures\n",
        "    accuracy  = cross_val_score(clf, X_train, y_train, cv = 10, scoring = 'accuracy')\n",
        "    f1_score = cross_val_score(clf, X_train, y_train, cv = 10, scoring = 'f1_macro')\n",
        "    # records the time\n",
        "    c_end = time.time()\n",
        "\n",
        "\n",
        "    # calculating mean of all 10 observation's accuracy and f1, taking percent and rounding to two decimal places\n",
        "    acc_mean = np.round(accuracy.mean() * 100, 2)\n",
        "    f1_mean = np.round(f1_score.mean() * 100, 2)\n",
        "\n",
        "\n",
        "    # substracts end time with start to give actual time taken in seconds\n",
        "    # divides by 60 to convert in minutes and rounds the answer to three decimal places\n",
        "    # time in training\n",
        "    t_time = np.round((t_end - t_start) / 60, 3)\n",
        "    # time for evaluating scores\n",
        "    c_time = np.round((c_end - c_start) / 60, 3)\n",
        "\n",
        "\n",
        "    # Removing traces of classifier\n",
        "    clf = None\n",
        "\n",
        "\n",
        "    # returns performance measure and time of the classifier\n",
        "    print(\"The accuracy score of this classifier on our training set is\", acc_mean,\"% and f1 score is\", f1_mean,\"% taking\", t_time,\"minutes to train and\", c_time,\n",
        "          \"minutes to evaluate cross validation and metric scores.\")"
      ],
      "metadata": {
        "id": "Zq82-JbXuA2g"
      },
      "id": "Zq82-JbXuA2g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing Random Forest function\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model_evaluation(RandomForestClassifier(n_jobs=-1, random_state = 53))"
      ],
      "metadata": {
        "id": "VOdbivKquJ2m"
      },
      "id": "VOdbivKquJ2m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing K-Nearest Neighbors Classifier function\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model_evaluation(KNeighborsClassifier(n_jobs=-1))"
      ],
      "metadata": {
        "id": "3OAVyRAwuHQG"
      },
      "id": "3OAVyRAwuHQG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing Multinomial classifier, one of the Naive Bayes classifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# passing the model to function to get performance measures\n",
        "model_evaluation(MultinomialNB())"
      ],
      "metadata": {
        "id": "7tsoMmfYuE12"
      },
      "id": "7tsoMmfYuE12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing Stochastic Gradient Descent Classifier function\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "model_evaluation(SGDClassifier(n_jobs=-1, random_state = 53))"
      ],
      "metadata": {
        "id": "otenTQJhuL0G"
      },
      "id": "otenTQJhuL0G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing AdaBoost classifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "model_evaluation(ExtraTreesClassifier(n_jobs=-1, random_state = 53))"
      ],
      "metadata": {
        "id": "6fdCtIQfuOB_"
      },
      "id": "6fdCtIQfuOB_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model_evaluation(LogisticRegression(n_jobs = -1, random_state = 53))"
      ],
      "metadata": {
        "id": "dk57-tCauQFw"
      },
      "id": "dk57-tCauQFw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing EM scores for model performance measure\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# definning best chosen classifier\n",
        "clf = RandomForestClassifier(n_estimators = 50, random_state = 53)\n",
        "\n",
        "# training our model\n",
        "clf = clf.fit(X_train, y_train)\n",
        "\n",
        "# predicting unseen data\n",
        "predict = clf.predict(X_test)\n",
        "\n",
        "# calculating accuracy\n",
        "accuracy = accuracy_score(y_test, predict)\n",
        "\n",
        "# calculating f1 score\n",
        "f1_score = f1_score(y_test, predict, average = 'macro')\n",
        "\n",
        "# taking precentage and rounding to 3 places\n",
        "accuracy = np.round(accuracy * 100, 3)\n",
        "f1_score = np.round(f1_score * 100, 3)\n",
        "\n",
        "# cleaning traces\n",
        "clf = None\n",
        "\n",
        "# results\n",
        "print(\"The accuracy score of our final model Random Forest Classifier on our testing set is\", accuracy,\"% and f1 score is\", f1_score,\"%.\")"
      ],
      "metadata": {
        "id": "ziYinkoquXYQ"
      },
      "id": "ziYinkoquXYQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7cedd478",
      "metadata": {
        "id": "7cedd478"
      },
      "source": [
        "\n",
        "## Key Insights\n",
        "1. Dataset contains no missing values.\n",
        "2. Cover_Type is imbalanced.\n",
        "3. Elevation shows separation among classes.\n",
        "4. Some distance features are skewed.\n",
        "5. Correlation exists among several variables.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}